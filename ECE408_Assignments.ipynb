{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMai66yJXTobxQ4w7TNx+PA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AshmitB05/cuda-parallel-programming-ece408/blob/main/ECE408_Assignments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7m7heIlzw4Y",
        "outputId": "cb9b8e6b-3d90-4b38-e2a0-efbac0d4e578"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nvcc4jupyter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-83EuOBp5haG",
        "outputId": "d301f74b-8f62-4f33-a700-5de7fe3893aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvcc4jupyter\n",
            "  Downloading nvcc4jupyter-1.2.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Downloading nvcc4jupyter-1.2.1-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: nvcc4jupyter\n",
            "Successfully installed nvcc4jupyter-1.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext nvcc4jupyter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8lKN82iz6Ke",
        "outputId": "4e2e5083-4231-4b82-8cd7-b271c5dcc4b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected platform \"Colab\". Running its setup...\n",
            "Source files will be saved in \"/tmp/tmp1rua9its\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect selected GPU and its NVIDA architecture:\n",
        "import subprocess\n",
        "gpu_info = subprocess.getoutput(\"nvidia-smi --query-gpu=name,compute_cap --format=csv,noheader,nounits\")\n",
        "if \"not found\" in gpu_info.lower(): raise RuntimeError(\"Error: No GPU found. Please select a GPU runtime environment.\")\n",
        "gpu_name, compute_cap = map(str.strip, gpu_info.split(','))\n",
        "gpu_arch = f\"sm_{compute_cap.replace('.', '')}\"\n",
        "\n",
        "print(f\"{'GPU Name':<15}: {gpu_name}\")\n",
        "print(f\"{'Architecture':<15}: {gpu_arch}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bARje_udz7Br",
        "outputId": "5e5c99c2-ce2e-4ebb-9527-6deb8fd7d56c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Name       : Tesla T4\n",
            "Architecture   : sm_75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda -c \"--gpu-architecture $gpu_arch\"\n",
        "#include <stdio.h>\n",
        "\n",
        "int main(){\n",
        "int devcount;\n",
        "cudaGetDeviceCount(&devcount);\n",
        "printf(\"Number of devices: %d\\n\", devcount);\n",
        "cudaDeviceProp prop;\n",
        "for(int i=0;i<devcount;i++){\n",
        "    cudaGetDeviceProperties(&prop,i);\n",
        "    printf(\"Device %d: %s\\n\",i,prop.name);\n",
        "    printf(\"Device %d: compute capablity %d.%d\\n\",i,prop.major,prop.minor);\n",
        "    printf(\"Device %d: Warp size %d\\n\",i,prop.warpSize);\n",
        "    printf(\"Device %d: Max threads per block %d x %d y %d z\\n\",i,prop.maxThreadsDim[0],prop.maxThreadsDim[1],prop.maxThreadsDim[2]);\n",
        "    printf(\"Device %d: Max blocks per grid %d x %d y %d z\\n\",i,prop.maxGridSize[0],prop.maxGridSize[1],prop.maxGridSize[2]);\n",
        "    printf(\"Device %d: Global Memory size %d kb \\n\",i,prop.totalGlobalMem/1024);\n",
        "    printf(\"Device %d: Shared Memory per block %d kb \\n\",i,prop.sharedMemPerBlock/1024);\n",
        "    printf(\"Device %d: Constant Memory %d kb \\n\",i,prop.totalConstMem/1024);\n",
        "    printf(\"Device %d: Max threads per multiprocessor %d \\n\",i,prop.maxThreadsPerMultiProcessor);\n",
        "    printf(\"Device %d: Registers per block %d\\n\",i,prop.regsPerBlock);\n",
        "    printf(\"clock rate %d\\n\",prop.clockRate);\n",
        "    return 0;\n",
        "\n",
        "\n",
        "\n",
        "}}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnOWfMxGz9-v",
        "outputId": "f870fb47-6879-4bc3-ae28-1f0931839b8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of devices: 1\n",
            "Device 0: Tesla T4\n",
            "Device 0: compute capablity 7.5\n",
            "Device 0: Warp size 32\n",
            "Device 0: Max threads per block 1024 x 1024 y 64 z\n",
            "Device 0: Max blocks per grid 2147483647 x 65535 y 65535 z\n",
            "Device 0: Global Memory size 15457344 kb \n",
            "Device 0: Shared Memory per block 48 kb \n",
            "Device 0: Constant Memory 64 kb \n",
            "Device 0: Max threads per multiprocessor 1024 \n",
            "Device 0: Registers per block 65536\n",
            "clock rate 1590000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#MP0 Sum of an array\n",
        "\n",
        "%%cuda -c \"--gpu-architecture $gpu_arch\"\n",
        "#include <stdio.h>\n",
        "#define N 1000\n",
        "__global__ void kernel(int *a,int *b,int *c,int n){\n",
        "    int i=blockIdx.x*blockDim.x+threadIdx.x;\n",
        "    if(i<n){\n",
        "        c[i]=a[i]+b[i];\n",
        "    }\n",
        "}\n",
        "int main(){\n",
        "    int a[N],b[N],c[N];\n",
        "    int *da,*db,*dc;\n",
        "    cudaMalloc((void**)&da,N*sizeof(int));\n",
        "    cudaMalloc((void**)&db,N*sizeof(int));\n",
        "    cudaMalloc((void**)&dc,N*sizeof(int));\n",
        "    for(int i=0;i<N;i++){\n",
        "        a[i]=i;\n",
        "        b[i]=i;\n",
        "    }\n",
        "    cudaEvent_t start,end;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&end);\n",
        "    cudaEventRecord(start);\n",
        "    cudaMemcpy(da,a,N*sizeof(int),cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(db,b,N*sizeof(int),cudaMemcpyHostToDevice);\n",
        "    dim3 grid(ceil(N/256.0),1,1);\n",
        "    dim3 block(256,1,1);\n",
        "    kernel<<<grid,block>>>(da,db,dc,N);\n",
        "    cudaMemcpy(c,dc,N*sizeof(int),cudaMemcpyDeviceToHost);\n",
        "    cudaEventRecord(end);\n",
        "    cudaEventSynchronize(end);\n",
        "    float time=0;\n",
        "    cudaEventElapsedTime(&time,start,end);\n",
        "    printf(\"GPU Time taken: %f ms\\n\",time);\n",
        "    cudaDeviceSynchronize();\n",
        "    cudaEventDestroy(start);\n",
        "    cudaEventDestroy(end);\n",
        "    int d[N];\n",
        "    cudaEvent_t start1,end1;\n",
        "    cudaEventCreate(&start1);\n",
        "    cudaEventCreate(&end1);\n",
        "    cudaEventRecord(start1);\n",
        "\n",
        "    for(int i=0;i<N;i++){\n",
        "        d[i]=a[i]+b[i];\n",
        "    }\n",
        "    cudaEventRecord(end1);\n",
        "    cudaEventSynchronize(end1);\n",
        "    float time1=0;\n",
        "    cudaEventElapsedTime(&time1,start1,end1);\n",
        "    printf(\"CPU Time taken: %f ms\\n\",time1);\n",
        "    int count=0;\n",
        "    for(int i=0;i<N;i++){\n",
        "        if(c[i]!=d[i]){\n",
        "            count++;\n",
        "            printf(\"%d %d , %d\\n\", i,c[i],d[i]);\n",
        "        }\n",
        "    }\n",
        "    if(count==0){\n",
        "        printf(\"Correct\\n\");\n",
        "     }\n",
        "    else{\n",
        "        printf(\"failure\\n\");\n",
        "    }\n",
        "    cudaFree(da);\n",
        "    cudaFree(db);\n",
        "    cudaFree(dc);\n",
        "\n",
        "    return 0;\n",
        "\n",
        "\n",
        "\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHIuEx0T_Hwh",
        "outputId": "39daaf43-4e74-4dc1-d0c0-cd9b253f7398"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Time taken: 1.166688 ms\n",
            "CPU Time taken: 0.005056 ms\n",
            "Correct\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#MP1 mat mul\n",
        "\n",
        "\n",
        "%%cuda -c \"--gpu-architecture $gpu_arch\"\n",
        "#define l 64\n",
        "#define r 66\n",
        "#define width 65\n",
        "#include <stdio.h>\n",
        "__global__ void kernel(int *a,int *b,int *c){\n",
        "    int row=blockIdx.y*blockDim.y+threadIdx.y;\n",
        "    int col=blockIdx.x*blockDim.x+threadIdx.x;\n",
        "\n",
        "    if(row<l && col<r){\n",
        "        int p=0;\n",
        "        for(int i=0;i<width;i++){\n",
        "            p+=a[row*width+i]*b[i*r+col];\n",
        "        }\n",
        "        c[row*r+col]=p;\n",
        "    }\n",
        "\n",
        "}\n",
        "int main(){\n",
        "    int a[l*width],b[width*r],c[l*r],d[l*r];\n",
        "    for(int i=0;i<l;i++){\n",
        "        for(int j=0;j<width;j++){\n",
        "            a[i*width+j]=i*width+j;\n",
        "        }\n",
        "    }\n",
        "    for(int i=0;i<width;i++){\n",
        "        for(int j=0;j<r;j++){\n",
        "            b[i*r+j]=i*r+j;\n",
        "        }\n",
        "\n",
        "  }\n",
        "    cudaEvent_t start,end;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&end);\n",
        "    cudaEventRecord(start);\n",
        "    for(int i=0;i<l;i++){\n",
        "        for(int j=0;j<r;j++){\n",
        "            for(int k=0;k<width;k++){\n",
        "                d[i*r+j]+=a[i*width+k]*b[k*r+j];\n",
        "        }\n",
        "\n",
        "    }}\n",
        "    cudaEventRecord(end);\n",
        "    cudaEventSynchronize(end);\n",
        "    float time=0;\n",
        "    cudaEventElapsedTime(&time,start,end);\n",
        "    printf(\"CPU Time taken: %f ms\\n\",time);\n",
        "    int *da,*db,*dc;\n",
        "    cudaMalloc((void**)&da,l*width*sizeof(int));\n",
        "    cudaMalloc((void**)&db,width*r*sizeof(int));\n",
        "    cudaMalloc((void**)&dc,l*r*sizeof(int));\n",
        "    cudaEvent_t start1,end1;\n",
        "    cudaEventCreate(&start1);\n",
        "    cudaEventCreate(&end1);\n",
        "    cudaEventRecord(start1);\n",
        "    cudaMemcpy(da,a,l*width*sizeof(int),cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(db,b,width*r*sizeof(int),cudaMemcpyHostToDevice);\n",
        "    dim3 grid(ceil(r/16.0),ceil(l/16.0),1);\n",
        "    dim3 block(16,16,1);\n",
        "    kernel<<<grid,block>>>(da,db,dc);\n",
        "    cudaMemcpy(c,dc,l*r*sizeof(int),cudaMemcpyDeviceToHost);\n",
        "    cudaEventRecord(end1);\n",
        "    cudaEventSynchronize(end1);\n",
        "    float time1=0;\n",
        "    cudaEventElapsedTime(&time1,start1,end1);\n",
        "    printf(\"GPU Time taken: %f ms\\n\",time1);\n",
        "    int count=0;\n",
        "    for(int i=0;i<l;i++){\n",
        "        for(int j=0;j<r;j++){\n",
        "            if(c[i*r+j]!=d[i*r+j]){\n",
        "                count++;\n",
        "                printf(\"%d %d %d , %d\\n\", i,j,c[i*r+j],d[i*r+j]);\n",
        "            }}}\n",
        "    if(count==0){\n",
        "        printf(\"Correct\\n\");\n",
        "     }\n",
        "    else{\n",
        "        printf(\"failure\\n\");}\n",
        "    cudaFree(da);\n",
        "    cudaFree(db);\n",
        "    cudaFree(dc);\n",
        "    return 0;\n",
        "\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZ3H-QklijU5",
        "outputId": "e825ede3-368d-4aab-e2ef-b3e92480de85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU Time taken: 1.716224 ms\n",
            "GPU Time taken: 3.714336 ms\n",
            "Correct\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#MP2 tiled mat mul\n",
        "\n",
        "%%cuda -c \"--gpu-architecture $gpu_arch\"\n",
        "#define l 4\n",
        "#define r 4\n",
        "#define width 3\n",
        "#define tile 2\n",
        "#include <stdio.h>\n",
        "__global__ void kernel(int *a,int *b,int *c){\n",
        "    int row=blockIdx.y*blockDim.y+threadIdx.y;\n",
        "    int col=blockIdx.x*blockDim.x+threadIdx.x;\n",
        "    __shared__ int Mds[tile][tile];\n",
        "    __shared__ int Nds[tile][tile];\n",
        "    int p=0;\n",
        "    for(int i=0;i<((width/tile)+1);i++){\n",
        "        if(row<l && (i*tile+threadIdx.x)<width){\n",
        "            Mds[threadIdx.y][threadIdx.x]=a[row*width+i*tile+threadIdx.x];\n",
        "        }\n",
        "        else{\n",
        "             Mds[threadIdx.y][threadIdx.x]=0;\n",
        "        }\n",
        "        if(col<r && (i*tile+threadIdx.y)<width){\n",
        "            Nds[threadIdx.y][threadIdx.x]=b[(i*tile+threadIdx.y)*r+col];\n",
        "        }\n",
        "        else{\n",
        "            Nds[threadIdx.y][threadIdx.x]=0;\n",
        "\n",
        "        }\n",
        "        __syncthreads();\n",
        "\n",
        "        for(int k=0;k<tile;k++){\n",
        "            p+=Mds[threadIdx.y][k]*Nds[k][threadIdx.x];\n",
        "        }\n",
        "        __syncthreads();\n",
        "\n",
        "        }\n",
        "    if(row<l && col<r){\n",
        "            c[row*r+col]=p;\n",
        "        }\n",
        "\n",
        "}\n",
        "int main(){\n",
        "    int a[l*width],b[width*r],c[l*r],d[l*r];\n",
        "    for(int i=0;i<l;i++){\n",
        "        for(int j=0;j<width;j++){\n",
        "            a[i*width+j]=i*width+j;\n",
        "        }\n",
        "    }\n",
        "    for(int i=0;i<width;i++){\n",
        "        for(int j=0;j<r;j++){\n",
        "            b[i*r+j]=i*r+j;\n",
        "        }\n",
        "\n",
        "  }\n",
        "    cudaEvent_t start,end;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&end);\n",
        "    cudaEventRecord(start);\n",
        "    for(int i=0;i<l;i++){\n",
        "        for(int j=0;j<r;j++){\n",
        "            for(int k=0;k<width;k++){\n",
        "                d[i*r+j]+=a[i*width+k]*b[k*r+j];\n",
        "        }\n",
        "\n",
        "    }}\n",
        "    cudaEventRecord(end);\n",
        "    cudaEventSynchronize(end);\n",
        "    float time=0;\n",
        "    cudaEventElapsedTime(&time,start,end);\n",
        "    printf(\"CPU Time taken: %f ms\\n\",time);\n",
        "    int *da,*db,*dc;\n",
        "    cudaMalloc((void**)&da,l*width*sizeof(int));\n",
        "    cudaMalloc((void**)&db,width*r*sizeof(int));\n",
        "    cudaMalloc((void**)&dc,l*r*sizeof(int));\n",
        "    cudaEvent_t start1,end1;\n",
        "    cudaEventCreate(&start1);\n",
        "    cudaEventCreate(&end1);\n",
        "    cudaEventRecord(start1);\n",
        "    cudaMemcpy(da,a,l*width*sizeof(int),cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(db,b,width*r*sizeof(int),cudaMemcpyHostToDevice);\n",
        "    dim3 grid(ceil(r/2.0),ceil(l/2.0),1);\n",
        "    dim3 block(2,2,1);\n",
        "    kernel<<<grid,block>>>(da,db,dc);\n",
        "    cudaMemcpy(c,dc,l*r*sizeof(int),cudaMemcpyDeviceToHost);\n",
        "    cudaEventRecord(end1);\n",
        "    cudaEventSynchronize(end1);\n",
        "    float time1=0;\n",
        "    cudaEventElapsedTime(&time1,start1,end1);\n",
        "    printf(\"GPU Time taken: %f ms\\n\",time1);\n",
        "    int count=0;\n",
        "    for(int i=0;i<l;i++){\n",
        "        for(int j=0;j<r;j++){\n",
        "            if(c[i*r+j]!=d[i*r+j]){\n",
        "                count++;\n",
        "                printf(\"%d %d %d , %d\\n\", i,j,c[i*r+j],d[i*r+j]);\n",
        "            }}}\n",
        "    if(count==0){\n",
        "        printf(\"Correct\\n\");\n",
        "     }\n",
        "    else{\n",
        "        printf(\"failure\\n\");}\n",
        "    cudaFree(da);\n",
        "    cudaFree(db);\n",
        "    cudaFree(dc);\n",
        "    return 0;\n",
        "\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89560316-7acd-4236-a307-9e722a5015cb",
        "id": "sJ_u-NNIKXSA"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU Time taken: 0.002720 ms\n",
            "GPU Time taken: 0.158112 ms\n",
            "Correct\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#MP3 3d convo\n",
        "\n",
        "%%cuda -c \"--gpu-architecture $gpu_arch\"\n",
        "#include <stdio.h>\n",
        "#define l 16\n",
        "#define width 5\n",
        "#define rand1 4\n",
        "#define outdim 4\n",
        "#define indim 8\n",
        "#define r 2\n",
        "__constant__ float dfilter[width][width][width];\n",
        "__global__ void kernel1(float *input,float *output){\n",
        "    int row=blockIdx.y*blockDim.y+threadIdx.y;\n",
        "    int col=blockIdx.x*blockDim.x+threadIdx.x;\n",
        "    int z=blockIdx.z*blockDim.z+threadIdx.z;\n",
        "    float p=0.0;\n",
        "    for(int fz=0;fz<width;fz++){\n",
        "        for(int fy=0;fy<width;fy++){\n",
        "            for(int fx=0;fx<width;fx++){\n",
        "                int inz=z-width/2+fz;\n",
        "                int inx=col-width/2+fx;\n",
        "                int iny=row-width/2+fy;\n",
        "                if(inz>=0 && inx>=0 && iny>=0 && inz<l && inx<l && iny<l){\n",
        "                    p=p+dfilter[fx][fy][fz]*input[inz*l*l+iny*l+inx];\n",
        "\n",
        "                }\n",
        "\n",
        "    }}}\n",
        "    if(row>=0 && col>=0 && z>=0 && row<l && col<l && z<l){\n",
        "    output[z*l*l+row*l+col]=p;}\n",
        "}\n",
        "__global__ void kernel2(float *input,float *output){\n",
        "  int row=blockIdx.y*outdim+threadIdx.y-r;\n",
        "  int col=blockIdx.x*outdim+threadIdx.x-r;\n",
        "  int z=blockIdx.z*outdim+threadIdx.z-r;\n",
        "  __shared__ float Ns[indim][indim][indim];\n",
        "  if(row>=0 && col>=0 && z>=0 && row<l && col<l && z<l){\n",
        "      Ns[threadIdx.z][threadIdx.y][threadIdx.x]=input[z*l*l+row*l+col];\n",
        "  }\n",
        "  else{\n",
        "      Ns[threadIdx.z][threadIdx.y][threadIdx.x]=0;\n",
        "  }\n",
        "  __syncthreads();\n",
        "\n",
        "  int tilerow=threadIdx.y-r;\n",
        "  int tilecol=threadIdx.x-r;\n",
        "  int tilez=threadIdx.z-r;\n",
        "  if(row>=0 && z>=0 && col>=0 && row<l && col<l && z<l){\n",
        "  if(tilerow>=0 && tilecol>=0 && tilez>=0 && tilerow<outdim && tilecol<outdim && tilez<outdim){\n",
        "       float p=0.0;\n",
        "       for(int fz=0;fz<width;fz++){\n",
        "        for(int fy=0;fy<width;fy++){\n",
        "            for(int fx=0;fx<width;fx++){\n",
        "                p=p+dfilter[fx][fy][fz]*Ns[tilez+fz][tilerow+fy][tilecol+fx];\n",
        "\n",
        "              }}}\n",
        "\n",
        "       output[z*l*l+row*l+col]=p;\n",
        "  }}\n",
        "}\n",
        "\n",
        "int main(){\n",
        "    printf(\"start\");\n",
        "    float input[l*l*l];\n",
        "    float filter[width][width][width];\n",
        "    float output1[l*l*l];\n",
        "    float output2[l*l*l];\n",
        "    for(int i=0;i<l;i++){\n",
        "        for(int j=0;j<l;j++){\n",
        "            for(int k=0;k<l;k++){\n",
        "                input[i*(l*l)+j*l+k]=10.0;\n",
        "        }\n",
        "  }}\n",
        "    for(int i=0;i<width;i++){\n",
        "        for(int j=0;j<width;j++){\n",
        "            for(int k=0;k<width;k++){\n",
        "                filter[i][j][k]=0.5;\n",
        "        }\n",
        "  }}\n",
        "    printf(\"input done\");\n",
        "    float *dinput,*dfilter,*doutput1,*doutput2;\n",
        "    cudaMalloc((void**)&dinput,l*l*l*sizeof(float));\n",
        "\n",
        "    cudaMalloc((void**)&doutput1,l*l*l*sizeof(float));\n",
        "    cudaMalloc((void**)&doutput2,l*l*l*sizeof(float));\n",
        "    cudaMemcpy(dinput,input,l*l*l*sizeof(float),cudaMemcpyHostToDevice);\n",
        "    cudaMemcpyToSymbol(dfilter,filter,width*width*width*sizeof(float));\n",
        "    dim3 grid(ceil(l/16.0),ceil(l/16.0),ceil(l/16.0));\n",
        "    dim3 block(16,16,16);\n",
        "    cudaEvent_t start1,end1;\n",
        "    cudaEventCreate(&start1);\n",
        "    cudaEventCreate(&end1);\n",
        "    cudaEventRecord(start1);\n",
        "    kernel1<<<grid,block>>>(dinput,doutput1);\n",
        "    cudaMemcpy(output1,doutput1,l*l*l*sizeof(float),cudaMemcpyDeviceToHost);\n",
        "    cudaEventRecord(end1);\n",
        "    cudaEventSynchronize(end1);\n",
        "    float time1=0;\n",
        "    cudaEventElapsedTime(&time1,start1,end1);\n",
        "    printf(\"GPU Time taken for naive : %f ms\\n\",time1);\n",
        "    printf(\"naive done\");\n",
        "    dim3 grid1(ceil(l/8.0),ceil(l/8.0),ceil(l/8.0));\n",
        "    dim3 block1(8,8,8);\n",
        "    cudaEvent_t start2,end2;\n",
        "    cudaEventCreate(&start2);\n",
        "    cudaEventCreate(&end2);\n",
        "    cudaEventRecord(start2);\n",
        "    kernel2<<<grid1,block1>>>(dinput,doutput2);\n",
        "    cudaMemcpy(output2,doutput2,l*l*l*sizeof(float),cudaMemcpyDeviceToHost);\n",
        "    cudaEventRecord(end2);\n",
        "    cudaEventSynchronize(end2);\n",
        "    float time2=0;\n",
        "    cudaEventElapsedTime(&time2,start2,end2);\n",
        "\n",
        "    printf(\"GPU time taken for shared: %f ms\\n\",time2);\n",
        "    int count=0;\n",
        "    for(int i=0;i<l;i++){\n",
        "        for(int j=0;j<l;j++){\n",
        "            for(int k=0;k<l;k++){\n",
        "                if(output1[i*l*l+j*l+k]!= output2[i*l*l+j*l+k]){\n",
        "                    count++;\n",
        "\n",
        "    }}}}\n",
        "\n",
        "    if(count==0){\n",
        "        printf(\"correct\");\n",
        "    }\n",
        "    else{\n",
        "        printf(\"incorrect\");\n",
        "    }\n",
        "    cudaFree(dinput);\n",
        "    cudaFree(dfilter);\n",
        "    cudaFree(doutput1);\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "xfylSPNDl8i0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3e18879-c2a6-4348-8719-219273cedf56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "startinput doneGPU Time taken for naive : 0.303712 ms\n",
            "naive doneGPU time taken for shared: 0.068160 ms\n",
            "correct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#MP4 sum reduction\n",
        "\n",
        "%%cuda -c \"--gpu-architecture $gpu_arch\"\n",
        "#define l 4090\n",
        "#define bx 1024\n",
        "#include <stdio.h>\n",
        "__global__ void kernel(int *a,int *output){\n",
        "    __shared__ int d_a[bx];\n",
        "    int segment=2*blockIdx.x*blockDim.x;\n",
        "    if((blockIdx.x*blockDim.x+threadIdx.x)<l){\n",
        "        d_a[threadIdx.x]=a[segment+threadIdx.x]+a[segment+threadIdx.x+blockDim.x];\n",
        "    }\n",
        "    else{\n",
        "        d_a[threadIdx.x]=0;\n",
        "    }\n",
        "    __syncthreads();\n",
        "    for(int stride=(blockDim.x)/2;stride>=1;stride/=2){\n",
        "        if(threadIdx.x<stride){\n",
        "            d_a[threadIdx.x]+=d_a[threadIdx.x+stride];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "    if(threadIdx.x==0){\n",
        "           atomicAdd(output,d_a[threadIdx.x])     ;\n",
        "     }\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "int main(){\n",
        "    int a[l];\n",
        "    int output[1];\n",
        "    for(int i=0;i<l;i++){\n",
        "        a[i]=1;\n",
        "    }\n",
        "    int *da;\n",
        "    int *doutput;\n",
        "    cudaMalloc((void**)&da,l*sizeof(int));\n",
        "    cudaMalloc((void**)&doutput,sizeof(int));\n",
        "    cudaMemcpy(da,a,l*sizeof(int),cudaMemcpyHostToDevice);\n",
        "    dim3 grid(ceil(l/2048.0),1,1);\n",
        "    dim3 block(1024,1,1);\n",
        "    kernel<<<grid,block>>>(da,doutput);\n",
        "    cudaMemcpy(output,doutput,sizeof(int),cudaMemcpyDeviceToHost);\n",
        "    printf(\"%d\",output[0]);\n",
        "    cudaFree(da);\n",
        "    cudaFree(doutput);\n",
        "    return 0;\n",
        "\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zldY7zoSXHGK",
        "outputId": "dcddb9a3-3fc8-45e9-bafa-406e260a7caf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4090\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#MP5\n",
        "%%cuda -c \"--gpu-architecture $gpu_arch\"\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define BLOCK_SIZE 1024\n",
        "#define SECTION_SIZE 2048 // 2 * BLOCK_SIZE\n",
        "\n",
        "\n",
        "__global__ void scan_kernel(float *input, float *output, float *global_sum, int n) {\n",
        "\n",
        "    __shared__ float temp[SECTION_SIZE];\n",
        "\n",
        "    int tx = threadIdx.x;\n",
        "    int bx = blockIdx.x;\n",
        "\n",
        "\n",
        "    int block_offset = bx * SECTION_SIZE;\n",
        "\n",
        "    // Load 2 elements per thread\n",
        "    int index1 = block_offset + (2 * tx);\n",
        "    int index2 = block_offset + (2 * tx) + 1;\n",
        "\n",
        "\n",
        "    if (index1 < n) temp[2 * tx]     = input[index1];\n",
        "    else            temp[2 * tx]     = 0.0f;\n",
        "\n",
        "    if (index2 < n) temp[2 * tx + 1] = input[index2];\n",
        "    else            temp[2 * tx + 1] = 0.0f;\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "\n",
        "    for (int stride = 1; stride <= BLOCK_SIZE; stride *= 2) {\n",
        "        int index = (tx + 1) * stride * 2 - 1;\n",
        "        if (index < SECTION_SIZE) {\n",
        "            temp[index] += temp[index - stride];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "\n",
        "    if (tx == 0) {\n",
        "        atomicAdd(global_sum, temp[SECTION_SIZE - 1]);\n",
        "        temp[SECTION_SIZE - 1] = 0; // Clear for Exclusive Scan\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "\n",
        "    for (int stride = BLOCK_SIZE; stride >= 1; stride /= 2) {\n",
        "        int index = (tx + 1) * stride * 2 - 1;\n",
        "        if (index < SECTION_SIZE) {\n",
        "            float t = temp[index - stride];\n",
        "            temp[index - stride] = temp[index];\n",
        "            temp[index] += t;\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "\n",
        "    if (index1 < n) output[index1] = temp[2 * tx];\n",
        "    if (index2 < n) output[index2] = temp[2 * tx + 1];\n",
        "}\n",
        "\n",
        "\n",
        "int main() {\n",
        "    int num_elements = 4096; // 2 blocks * 2048\n",
        "    size_t size = num_elements * sizeof(float);\n",
        "\n",
        "    // Host Memory\n",
        "    float *h_in = (float*)malloc(size);\n",
        "    float *h_out = (float*)malloc(size);\n",
        "    float h_global_sum_result = 0.0f;\n",
        "\n",
        "    // Initialize Input (1.0f everywhere)\n",
        "    for (int i = 0; i < num_elements; i++) {\n",
        "        h_in[i] = 1.0f;\n",
        "    }\n",
        "\n",
        "    // Device Memory\n",
        "    float *d_in, *d_out, *d_global_sum;\n",
        "    cudaMalloc((void**)&d_in, size);\n",
        "    cudaMalloc((void**)&d_out, size);\n",
        "    cudaMalloc((void**)&d_global_sum, sizeof(float));\n",
        "\n",
        "    // Initialize Global Sum to 0 on GPU\n",
        "    cudaMemset(d_global_sum, 0, sizeof(float));\n",
        "    cudaMemcpy(d_in, h_in, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Launch Config\n",
        "    dim3 dimGrid(2);\n",
        "    dim3 dimBlock(BLOCK_SIZE);\n",
        "\n",
        "    printf(\"Launching scan_kernel with atomicAdd...\\n\");\n",
        "\n",
        "    // Timer\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    cudaEventRecord(start);\n",
        "    scan_kernel<<<dimGrid, dimBlock>>>(d_in, d_out, d_global_sum, num_elements);\n",
        "    cudaEventRecord(stop);\n",
        "\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    float milliseconds = 0;\n",
        "    cudaEventElapsedTime(&milliseconds, start, stop);\n",
        "    printf(\"Kernel Time: %.4f ms\\n\", milliseconds);\n",
        "\n",
        "    // Copy back\n",
        "    cudaMemcpy(h_out, d_out, size, cudaMemcpyDeviceToHost);\n",
        "    cudaMemcpy(&h_global_sum_result, d_global_sum, sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "\n",
        "\n",
        "    // 1. Check Global Sum\n",
        "    float cpu_total_sum = 0.0f;\n",
        "    for(int i=0; i<num_elements; i++) cpu_total_sum += h_in[i];\n",
        "\n",
        "    printf(\"Global Sum: GPU=%.2f, CPU=%.2f\\n\", h_global_sum_result, cpu_total_sum);\n",
        "\n",
        "    // 2. Check Scan (Block Local Logic)\n",
        "    // Block 0 should be 0..2047. Block 1 should also be 0..2047 (because it's local scan)\n",
        "    bool scan_match = true;\n",
        "    for (int i = 0; i < num_elements; i++) {\n",
        "        // Expected value depends on block index\n",
        "        float expected = (float)(i % 2048);\n",
        "        if (fabs(h_out[i] - expected) > 0.01) {\n",
        "            printf(\"Mismatch at %d: Expected %.1f, Got %.1f\\n\", i, expected, h_out[i]);\n",
        "            scan_match = false;\n",
        "            break;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if (scan_match) printf(\"Scan Array: PASSED (Block-Local Logic)\\n\");\n",
        "    else            printf(\"Scan Array: FAILED\\n\");\n",
        "\n",
        "    // Cleanup\n",
        "    cudaFree(d_in);\n",
        "    cudaFree(d_out);\n",
        "    cudaFree(d_global_sum);\n",
        "    free(h_in);\n",
        "    free(h_out);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wWUWBXf5iT4",
        "outputId": "891f30a2-411c-4869-a520-b8eee82aadb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Launching scan_kernel with atomicAdd...\n",
            "Kernel Time: 0.1154 ms\n",
            "Global Sum: GPU=4096.00, CPU=4096.00\n",
            "Scan Array: PASSED (Block-Local Logic)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#MP7\n",
        "%%cuda -c \"--gpu-architecture $gpu_arch\"\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "\n",
        "#define ROWS 6\n",
        "#define COLS 6\n",
        "\n",
        "// --- JDS Matrix Structure ---\n",
        "typedef struct {\n",
        "    int num_rows;\n",
        "    int num_cols;\n",
        "    int max_row_len;    // Max non-zeros in any row (determines outer loop count)\n",
        "    int *jd_ptr;        // Pointers to the start of each jagged diagonal\n",
        "    int *col_index;     // Original column indices\n",
        "    float *data;        // The non-zero values\n",
        "    int *row_ptr;       // STORES ORIGINAL ROW INDICES (The permutation map)\n",
        "} JDSMatrix;\n",
        "\n",
        "// --- 1. CPU Reference (Dense Calculation) ---\n",
        "// This represents the standard \"Host\" calculation for verification\n",
        "void cpu_dense_mv(float matrix[ROWS][COLS], float *vector, float *out) {\n",
        "    for (int i = 0; i < ROWS; i++) {\n",
        "        float sum = 0.0f;\n",
        "        for (int j = 0; j < COLS; j++) {\n",
        "            sum += matrix[i][j] * vector[j];\n",
        "        }\n",
        "        out[i] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "// --- 2. GPU Simulation (JDS Kernel) ---\n",
        "// This implements the JDS logic exactly as a GPU thread would execute it\n",
        "void jds_spmv_kernel(JDSMatrix *mat, float *vector, float *out) {\n",
        "    // Initialize output vector to 0\n",
        "    for (int i = 0; i < mat->num_rows; i++) {\n",
        "        out[i] = 0.0f;\n",
        "    }\n",
        "\n",
        "    // Iterate through each Jagged Diagonal (Column in the shifted matrix)\n",
        "    for (int i = 0; i < mat->max_row_len; i++) {\n",
        "        int start_index = mat->jd_ptr[i];\n",
        "        int end_index   = mat->jd_ptr[i+1];\n",
        "\n",
        "        // Process elements in this jagged diagonal\n",
        "        for (int k = start_index; k < end_index; k++) {\n",
        "\n",
        "            // Calculate which row (0 to N) inside the JDS block we are processing\n",
        "            int row_in_jds_block = k - start_index;\n",
        "\n",
        "            // Use row_ptr to map back to the ORIGINAL row index\n",
        "            int original_row = mat->row_ptr[row_in_jds_block];\n",
        "\n",
        "            // Perform the multiplication\n",
        "            // out[original_row] += value * input_vector[column]\n",
        "            out[original_row] += mat->data[k] * vector[mat->col_index[k]];\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // --- SETUP HARDCODED DATA ---\n",
        "\n",
        "    // 1. Input Vector (All 1.0)\n",
        "    float vector[COLS] = {1.0, 1.0, 1.0, 1.0, 1.0, 1.0};\n",
        "\n",
        "    // 2. Reference Dense Matrix (6x6)\n",
        "    // Used only for the CPU check\n",
        "    float dense_matrix[ROWS][COLS] = {\n",
        "        {1, 0, 2, 0, 0, 0}, // Row 0 (2 nz)\n",
        "        {0, 3, 0, 4, 0, 5}, // Row 1 (3 nz)\n",
        "        {0, 0, 6, 0, 0, 0}, // Row 2 (1 nz)\n",
        "        {7, 8, 9, 0, 0, 0}, // Row 3 (3 nz)\n",
        "        {0, 0, 0, 0, 10,0}, // Row 4 (1 nz)\n",
        "        {0, 0, 0, 0, 0, 11} // Row 5 (1 nz)\n",
        "    };\n",
        "\n",
        "    // 3. JDS Compressed Format Data\n",
        "    // Sorted Rows: R1(len3), R3(len3), R0(len2), R2(1), R4(1), R5(1)\n",
        "\n",
        "    float data[] = {\n",
        "        3, 7, 1, 6, 10, 11,   // Jagged Diagonal 0 (Col 0 of shifted matrix)\n",
        "        4, 8, 2,              // Jagged Diagonal 1 (Col 1 of shifted matrix)\n",
        "        5, 9                  // Jagged Diagonal 2 (Col 2 of shifted matrix)\n",
        "    };\n",
        "\n",
        "    int col_index[] = {\n",
        "        1, 0, 0, 2, 4, 5,     // Orig Col indices for JD 0\n",
        "        3, 1, 2,              // Orig Col indices for JD 1\n",
        "        5, 2                  // Orig Col indices for JD 2\n",
        "    };\n",
        "\n",
        "    int jd_ptr[] = {0, 6, 9, 11}; // Start indices of JDs\n",
        "\n",
        "    // The Permutation Map (row_ptr)\n",
        "    // Maps the sorted JDS rows back to: Row 1, Row 3, Row 0, Row 2, Row 4, Row 5\n",
        "    int row_ptr[] = {1, 3, 0, 2, 4, 5};\n",
        "\n",
        "    // Fill Struct\n",
        "    JDSMatrix jds;\n",
        "    jds.num_rows = ROWS;\n",
        "    jds.num_cols = COLS;\n",
        "    jds.max_row_len = 3;\n",
        "    jds.data = data;\n",
        "    jds.col_index = col_index;\n",
        "    jds.jd_ptr = jd_ptr;\n",
        "    jds.row_ptr = row_ptr; // Sending the permutation map to the kernel\n",
        "\n",
        "    float cpu_result[ROWS];\n",
        "    float gpu_result[ROWS];\n",
        "\n",
        "    // --- EXECUTION (1 Iteration) ---\n",
        "\n",
        "    // 1. Run CPU Reference\n",
        "    cpu_dense_mv(dense_matrix, vector, cpu_result);\n",
        "\n",
        "    // 2. Run GPU (JDS Kernel)\n",
        "    jds_spmv_kernel(&jds, vector, gpu_result);\n",
        "\n",
        "\n",
        "\n",
        "    int passed = 1;\n",
        "    for (int i = 0; i < ROWS; i++) {\n",
        "        // Compare values\n",
        "        float diff = fabs(cpu_result[i] - gpu_result[i]);\n",
        "        int match = (diff < 0.001); // Tolerance check\n",
        "        if (!match) passed = 0;\n",
        "\n",
        "\n",
        "    }\n",
        "\n",
        "\n",
        "    if (passed) {\n",
        "        printf(\"FINAL RESULT: PASSED (Outputs Match)\\n\");\n",
        "    } else {\n",
        "        printf(\"FINAL RESULT: FAILED (Outputs Do Not Match)\\n\");\n",
        "    }\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2mvT_AFL3wq",
        "outputId": "122b79f9-5617-4b6e-d181-e3bbf23fc1cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FINAL RESULT: PASSED (Outputs Match)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}